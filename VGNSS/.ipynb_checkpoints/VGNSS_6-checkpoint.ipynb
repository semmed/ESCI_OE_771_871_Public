{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <style>\n",
    ".footer {\n",
    "  position: fixed;\n",
    "  left: 0;\n",
    "  bottom: 0;\n",
    "  width: 100%;\n",
    "  background-color: white;\n",
    "  color: grey;\n",
    "  text-align: left;\n",
    "  font-size: 8px;\n",
    "}\n",
    "</style>\n",
    "<style>\n",
    ".header {\n",
    "  position: fixed;\n",
    "  left: 0;\n",
    "  top: 0;\n",
    "  width: 100%;\n",
    "  background-color: white;\n",
    "  color: grey;\n",
    "  text-align: center;\n",
    "  font-size: 8px;\n",
    "}\n",
    "</style>\n",
    "<div class=\"header\">\n",
    "    <img align=\"left\" width=\"4%\" style=\"padding-right:10px;\" src=\"../Images/Ccom.png\">\n",
    "    <p>Being a VGNSS Receiver</p>\n",
    "</div>\n",
    "\n",
    "<img align=\"left\" width=\"30%\" style=\"padding-right:10px;\" src=\"../Images/Ccom.png\">\n",
    "\n",
    "___\n",
    "# Computational Problem Set - Being a VGNSS Receiver\n",
    "\n",
    "## Step 6: Solving the Unknown Position by Using the Non-Linear Parametric Method of the Least Squares\n",
    "\n",
    "## Semme J. Dijkstra\n",
    "\n",
    "<div class=\"footer\">\n",
    "    <br><p>VGNSS Step 6: Semme J. Dijkstra April 23, 2020\n",
    "    </p>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an iterative non-linear parametric least squares GNSS position fix that converges to a solution. In other words repeatedly estimate $\\hat{\\vec x}$ so that:\n",
    "\n",
    "$$ A^TC_{obs}^{-1}A\\vec\\delta+A^TC_{obs}^{-1}\\vec w=\\vec0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-semmed/ESCI_OE_771_871_Clean\n",
      "/home/jupyter-semmed/ESCI_OE_771_871_Clean\n",
      "Reading SP3 file: ../mydata/igs13381.sp3\n",
      "Reading SP3 file: ../mydata/igl13381.sp3\n",
      "Reading rinex nav file: ../mydata/brdc2410.05n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from copy import copy\n",
    "from numpy import cos,pi,sin,pi,arccos, tan, arctan, arctan2, sqrt\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "vgnss_path=Path('../') # Get the path to the folder containing the mycode folder\n",
    "\n",
    "print(vgnss_path.resolve())\n",
    "sys.path.append(str(vgnss_path.resolve())) # add the folder to the list of paths \n",
    "from mycode.gnss import GNSS\n",
    "from mycode.sp3 import SP3\n",
    "# Start by running your previous steps - suppress the output by setting verbose to False\n",
    "\n",
    "import VGNSS_ALL_STEPS_1_5_CODE\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_background(color):    \n",
    "    script = (\n",
    "        \"var cell = this.closest('.jp-CodeCell');\"\n",
    "        \"var editor = cell.querySelector('.jp-Editor');\"\n",
    "        \"editor.style.background='{}';\"\n",
    "        \"this.parentNode.removeChild(this)\"\n",
    "    ).format(color)\n",
    "\n",
    "    display(HTML('<img src onerror=\"{}\">'.format(script)))\n",
    "\n",
    "# from mycode.lsq import LSQ\n",
    "# from mycode.ephemeris import Ephemeris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-semmed/ESCI_OE_771_871_Clean\n",
      "Reading SP3 file: ../mydata/igs13381.sp3\n",
      "Reading SP3 file: ../mydata/igl13381.sp3\n",
      "Reading rinex nav file: ../mydata/brdc2410.05n\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run VGNSS_ALL_STEPS_1_5_CODE\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Calculate the Covariance Matrix $C_l$ of the Observations $l^o$\n",
    "\n",
    "In the code cell below use the a-priori measurement variance $\\sigma_0^2$ assigned to `var_a_priori` in step 5.0.7 for the computation of fix uncertainty $C_l$ by propagating it using the `n_svs` x `n_svs` measurement covariance matrix `Cl`.\n",
    "\n",
    "$$C_{l,i} = \\sigma_i^2 *I$$\n",
    "\n",
    "Note that $ \\sigma_0^2$ is the a-priori variance estimate `var_a_priori`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cl[-1] = \n",
      "    267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00 \n",
      "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27 \n"
     ]
    }
   ],
   "source": [
    "## 6.0 Calculate the Covariance Matrix 𝐶𝑙 of the Observations 𝑙\n",
    "var_fac = [var_a_priori]\n",
    "Cl = []\n",
    "Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "\n",
    "if verbose:\n",
    "    print('    Cl[-1] = ')\n",
    "    for i in range(n_svs):\n",
    "        print('   ',end =\" \")\n",
    "        for j in range(n_svs):\n",
    "            print( '%5.2f'%(Cl[-1][i,j]),end =\" \"),\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked Example:\n",
    "    \n",
    "    Cl[-1] = \n",
    "    267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27  0.00 \n",
    "     0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00 267.27 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Calculate the Weight Matrix P\n",
    "\n",
    "Normally we weight observations by the inverse of their expected uncertainty i.e.,\n",
    "\n",
    "$$P_i= C_{l,i}^{-1}$$\n",
    "\n",
    "In the code cell below create the weight matrix `P` and assign it the inverse of the observation covariance matrix $C_l$ held by the variable `Cl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    P[-1] = \n",
      "    0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 \n",
      "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 \n"
     ]
    }
   ],
   "source": [
    "## 6.1 Calculate the Weight Matrix P\n",
    "P = [inv(Cl[-1])]\n",
    "\n",
    "if verbose:\n",
    "    print('    P[-1] = ')\n",
    "    for i in range(n_svs):\n",
    "        print('   ',end =\" \")\n",
    "        for j in range(n_svs):\n",
    "            print( '%.4f'%(P[-1][i,j]),end =\" \"),\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked Example:\n",
    "    \n",
    "    P[-1] = \n",
    "    0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 0.0000 \n",
    "    0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0037 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Calculate the Misclosures $\\vec w$ and Design $A$ of the Current Estimate $\\hat{\\vec x}$\n",
    "\n",
    "in this step we will create the lists `x_est`, `w` and `A` holding the successive estimates $\\hat{\\vec x}_i$, and the associate misclosures `\\vec w_i` and design matrices $A_i$\n",
    "\n",
    "The first elements are thus `x_est[0]` the initial estimate $\\hat{\\vec x}_0$ given by `xyzt_0` and the associated `w[0]` holding $\\vec w_0$ and `A[0]` holding $A_0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The initial misclosure vector w =\n",
      "    17987544954.607\n",
      "    17987545200.254\n",
      "    17987544739.435\n",
      "    17987546438.662\n",
      "    17987546139.732\n",
      "    17987547818.831\n",
      "    17987546413.403\n",
      "    17987546386.493\n",
      "    17987547858.883\n",
      "    17987546051.485\n",
      "    17987546084.444\n",
      "    17987545994.177\n",
      "    17987546413.321\n",
      "\n",
      "    The initial design matrix A =\n",
      "        0.672   0.418  -0.611 299792458.000\n",
      "       -0.846   0.096  -0.524 299792458.000\n",
      "       -0.652  -0.383  -0.654 299792458.000\n",
      "        0.746  -0.125  -0.654 299792458.000\n",
      "        0.090   0.459  -0.884 299792458.000\n",
      "       -0.052   0.806   0.589 299792458.000\n",
      "       -0.780   0.616   0.104 299792458.000\n",
      "       -0.446   0.786  -0.429 299792458.000\n",
      "        0.437   0.898   0.044 299792458.000\n",
      "       -0.846   0.524  -0.100 299792458.000\n",
      "        0.122   0.397  -0.910 299792458.000\n",
      "        0.587  -0.299  -0.752 299792458.000\n",
      "       -0.529   0.786  -0.319 299792458.000\n"
     ]
    }
   ],
   "source": [
    "## 6.2 Get the design matrix and misclosure vectors using the current estimate x_est[-1] and l_obs\n",
    "w = []  # Empty list to hold the misclosure vectors\n",
    "A = []  # Empty list to hold the design matrices\n",
    "Cx = [] # Empty list to hold the covariance matrices Cx\n",
    "\n",
    "# Add the initial estimate to the list x_est\n",
    "x_est = [xyzt_0]\n",
    "\n",
    "# Get the design A and misclosures w for x_est\n",
    "result = get_design_misclosure_range( x_est[-1], eph,l_obs, c, False)\n",
    "w.append(result[0])\n",
    "A.append(result[1])\n",
    "\n",
    "if verbose:\n",
    "    print('\\n    The initial misclosure vector w =')\n",
    "    for i in range(n_svs):\n",
    "        print('    %15.3f'%(w[-1][i]))\n",
    "    print('\\n    The initial design matrix A =')\n",
    "    for i in range(n_svs):\n",
    "        print('    %9.3f %7.3f %7.3f %7.3f'%(A[-1][i,0],A[-1][i,1],A[-1][i,2],A[-1][i,3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked Example:\n",
    "\n",
    "    The initial misclosure vector w =\n",
    "    17987544954.607\n",
    "    17987545200.254\n",
    "    17987544739.435\n",
    "    17987546438.662\n",
    "    17987546139.732\n",
    "    17987547818.831\n",
    "    17987546413.403\n",
    "    17987546386.493\n",
    "    17987547858.883\n",
    "    17987546051.485\n",
    "    17987546084.444\n",
    "    17987545994.177\n",
    "    17987546413.321\n",
    "\n",
    "    The initial design matrix A =\n",
    "        0.672   0.418  -0.611 299792458.000\n",
    "       -0.846   0.096  -0.524 299792458.000\n",
    "       -0.652  -0.383  -0.654 299792458.000\n",
    "        0.746  -0.125  -0.654 299792458.000\n",
    "        0.090   0.459  -0.884 299792458.000\n",
    "       -0.052   0.806   0.589 299792458.000\n",
    "       -0.780   0.616   0.104 299792458.000\n",
    "       -0.446   0.786  -0.429 299792458.000\n",
    "        0.437   0.898   0.044 299792458.000\n",
    "       -0.846   0.524  -0.100 299792458.000\n",
    "        0.122   0.397  -0.910 299792458.000\n",
    "        0.587  -0.299  -0.752 299792458.000\n",
    "       -0.529   0.786  -0.319 299792458.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Calculate Adjustment Variance Matrix $C_\\delta$\n",
    "\n",
    "in this step we will create the list `Cd`, holding the covariance $C_{d,i}$ of the successive adjustments $\\hat{\\vec\\delta}_i$\n",
    "\n",
    "The next estimate of the unknown $\\vec x_{i+1}$ and $\\vec\\delta_i$ share their covariance matrix: that is $C_{x,i+1}=C_{\\delta,i}$, which is the inverse of the **Normal Equations** N:\n",
    "\n",
    "$$ N = A^TC_l^{-1}A = A^TPA\\Rightarrow$$\n",
    "\n",
    "$$C_{\\delta,i}= C_{x,i+1} = \\left(A_i^TP_iA_i\\right)^{-1}$$\n",
    "\n",
    "In the code cell below calculate the covariance matrix $C_{d,0}$ of the current estimate $\\hat{\\vec\\delta}_0$ using the current design matrix `A[-1]` and misclosure vector `w[-1]` and add it to the list `Cd`.\n",
    "\n",
    "Note that the resulting variance and covariances associated to the time bias are zero i.e., the time is completely solved in the very first iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cd = \n",
      "             66.871          -1.538          26.608           0.000 \n",
      "             -1.538         186.641        -110.720          -0.000 \n",
      "             26.608        -110.720         190.127           0.000 \n",
      "              0.000          -0.000           0.000           0.000 \n"
     ]
    }
   ],
   "source": [
    "## 6.3 Calculate Adjustment Variance Matrix\n",
    "Cd = [] # Empty list to hold the covariance matrices Cd\n",
    "\n",
    "# Determine the covariance Cd of the *current* estimate of x\n",
    "Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "\n",
    "if verbose:\n",
    "    print('    Cd = ')\n",
    "    for i in range(len( x_est[-1])):\n",
    "        print('   ',end =\" \")\n",
    "        for j in range(len( x_est[-1])):\n",
    "            print( '%15.3f'%(Cd[-1][i,j]),end =\" \"),\n",
    "        print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Worked Example: \n",
    "     \n",
    "     Cd = \n",
    "             66.873          -1.540          26.611           0.000 \n",
    "             -1.540         186.640        -110.722          -0.000 \n",
    "             26.611        -110.722         190.130           0.000 \n",
    "              0.000          -0.000           0.000           0.000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Estimate the Adjustment Vector $\\hat\\delta$\n",
    "\n",
    "in this step we will create the list `delta`, holding the adjustments $\\delta_{i}$ for the successive estimates $\\hat{\\vec x}_i$\n",
    "\n",
    "\n",
    "$$\\hat{\\vec\\delta}_i = -\\left(A_i^TP_iA_i\\right)^{-1}A_i^TP_i\\vec w_i$$\n",
    "    \n",
    "using the adjustment covariance matrix $C_{\\delta,i}\\Rightarrow$\n",
    "\n",
    "$$\\hat{\\vec\\delta}_i = -C_{\\delta,i}A_i^TP_i\\vec w_i$$\n",
    "\n",
    "For this first iteration we then use `Cd[-1]`, `A[-1]`, `P` and, `w[-1]`, i.e., the current values.\n",
    "\n",
    "Note that adjustment vector $\\hat{\\vec\\delta}_i$, is the correction to $\\hat{ \\vec x}_i$, thus\n",
    "\n",
    "$$\\hat{ \\vec x}_{i+1} = \\hat{ \\vec x}_i+\\hat{\\vec\\delta}_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    delta = \n",
      "       -655.377\n",
      "       -805.803\n",
      "      -1178.345\n",
      "        -60.000\n"
     ]
    }
   ],
   "source": [
    "## 6.4 Estimate the Adjustment Vector delta\n",
    "# Create the list of the adjustments delta\n",
    "delta = []\n",
    "\n",
    "# Add the first adjustment to the list\n",
    "delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "\n",
    "if verbose:\n",
    "    print('    delta = ')\n",
    "    for i in range(len( delta[-1])):\n",
    "        print( '%15.3f'%(delta[-1][i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked example:\n",
    "\n",
    "    delta = \n",
    "       -655.377\n",
    "       -805.803\n",
    "      -1178.345\n",
    "        -60.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Estimate the Residual Vector $\\hat{\\vec r}$\n",
    "\n",
    "The residual $\\vec r$ is the difference between the unknown observables (i.e. what the ranges should be) and the observations $l^0$, as opposed to the misclosures $\\vec w$, which are the difference between the modeled observations $l$ and the observations $l^o$ themselves. Note that since $\\hat{ \\vec x}_{i+1} = \\hat{ \\vec x}_i+\\hat{\\vec\\delta}_i$  residuals $\\vec r_i$ calculated from $w_i$ and $A_i$ are associated to the ***next*** estimate $\\hat{ \\vec x}_{i+1}$! Since we have not updated the initial estimate yet the adjustments are still zero i.e.\n",
    "\n",
    "$$\\hat{\\vec r}_i = A_i\\hat{\\vec\\delta}_i  + \\vec w_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    r = \n",
      "      -1524.241\n",
      "       -125.905\n",
      "       -174.323\n",
      "        399.651\n",
      "        331.669\n",
      "         88.427\n",
      "       -115.254\n",
      "        130.757\n",
      "        376.328\n",
      "       -119.898\n",
      "        335.417\n",
      "        316.320\n",
      "         81.053\n"
     ]
    }
   ],
   "source": [
    "## 6.5 Estimate the Residual Vector\n",
    "r = [] # list of residual vectors\n",
    "\n",
    "r.append(A[-1]@delta[-1]+w[-1])\n",
    "\n",
    "if verbose:\n",
    "    print('    r = ')\n",
    "    for i in range(len( r[-1])):\n",
    "        print( '%15.3f'%(r[-1][i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked example:\n",
    "\n",
    "    r = \n",
    "      -1524.241\n",
    "       -125.905\n",
    "       -174.323\n",
    "        399.651\n",
    "        331.669\n",
    "         88.427\n",
    "       -115.254\n",
    "        130.757\n",
    "        376.328\n",
    "       -119.898\n",
    "        335.417\n",
    "        316.320\n",
    "         81.053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Compute the Variance Factor $\\sigma_i^2$\n",
    "\n",
    "Compute the variance factor $\\sigma_0^2$ using the estimated residual $\\hat{\\vec r}$. This variance vector is a potentially better estimate of the uncertainty associated to the observations as it is based on the residuals. The variance factor is the sum of the squares of the residuals divided by the degrees of freedom i.e,:\n",
    "\n",
    "$$ \\sigma_i^2 = \\dfrac{\\hat r_i^TC_{l,i}^{-1} \\hat r_i}{ (n - u)}= \\dfrac{\\hat r_i^TP_i \\hat r_i}{ (n - u)}$$\n",
    "\n",
    "where $n$ is the number of satellites used in the computation i.e., `n_svs`, and $u$ is the number of unknown parameters in $\\vec x$ i.e., the length of `x_est[0]`.\n",
    "\n",
    "In the code cell below calculate append the variance factor $\\sigma_i^2$ to the list `var_fac`. Note that this is the variance factor associated to the *current* iteration that may be used as the *a-priori* **uncertainty** for the next iteration. Note that in this case it will increase the uncertainty of the estimate, despite the estimate being closer to the truth, as there is the influence of the blunder that affects the estimate. Thus, if we see an increase in the uncertainty estimate after the first estimate this is an indication that error is present that is unaccounted for in the a-priori variance estimate. The two most common causes for this are the presence of blunders (as we know is the case here) and/or overly optimistic estimates of the accuracy of the observations. Thus this is a valuable tool to determine whether the instrument is performing according to expectation after gross errors have been eliminated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[35.62340853]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 6.6 Compute the Variance Factor 𝜎[𝑖]^2\n",
    "var_fac = []\n",
    "var_fac.append(r[-1].T@P[-1]@r/(n_svs - 4))\n",
    "sqrt(var_fac[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Create the Iterative Loop in which $\\hat{\\vec x}$ is Determined\n",
    "\n",
    "We estimate the unknown position and time $\\vec x$ by starting with an initial estimate $\\hat{\\vec x_0}$ and iteratively refining it by the adjustment $\\hat{\\vec\\delta}_i$. Theoretically we expect each iteration to produce a better estimate and thus smaller magnitude residual vector $\\hat{\\vec r_i}^TP\\hat{\\vec r_i}$. For numeric reasons this is *not* the case, this we should stop if $\\hat{\\vec r_i}^TP\\hat{\\vec r_i}$ starts dithering i.e., successively increasing and decreasing. As we are minimizing the sum of the squared residuals the first time $\\hat{\\vec r_i}^TP\\hat{\\vec r_i}$ increases we have converged to a solution.\n",
    "\n",
    "Note that fact that we can keep on computing until $\\hat{\\vec\\delta}_i$ has converged does *not* mean that we should. At some points the adjustments $\\hat{\\vec\\delta}_i$ become insignificant and just take unneeded resources to compute. To address this we normally predefine a convergence criterion for the norm of $\\hat{\\vec\\delta}_i$ , a standard by which we decide that whether a solution is sufficiently close, regardless whether is has numerically converged. Note that this has the potential of **significantly** reducing computing time.\n",
    "\n",
    "Both the above criteria are determined iteratively; sufficient convergence is not guaranteed either way. Therefore we should also have a mechanism to stop the iteration regardless the results. For a positioning problem such as the one presented here we normally expect to converge on a solution in three iterations or less. Thus, if we limit the number of iterations to 10 it is unlikely that we preemptively interrupt the adjustment procedure.\n",
    "\n",
    "We want to stop iterations stop after:\n",
    "    - 1) the magnitude of the residual vector `r` increases in magnitude, but only after the 2nd estimate\n",
    "    - 2) the magnitude of the adjustment vector `delta` becomes less than `conv_crit`\n",
    "    - 3) the loop has iterated `max_iter` times\n",
    "\n",
    "## 6.7.0 The Iterative Loop Initialization\n",
    "\n",
    "For both 1) and 2) we will set the value of the Boolean `converged` to `True` once the convergence has been reached. Before entering the loop we can check the magnitude of the current adjustment $\\hat{\\vec\\delta}_0$, but we can not compare it to a previous adjustment, as that does not yet exist. However, we do know the number of iterations, which at this point is zero (we still only have the estimate $\\hat{\\vec x}_0$)\n",
    "    \n",
    "Thus, at time we can set the value of `max_iter` for which we will use the value 10. We want to guarantee that we enter the iterative loop at least once, as that is where we set a number of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convergence criterion of 0.1mm was reached in 2 steps\n",
      "\n",
      "    var_fac[0] = 267.274\n",
      "    x_est[0] = \n",
      "      36916.951\n",
      "   -5514040.693\n",
      "    3194170.851\n",
      "          0.000\n",
      "\n",
      "    var_fac[1] = 1269.027\n",
      "    x_est[1] = \n",
      "      36916.951\n",
      "   -5514040.693\n",
      "    3194170.851\n",
      "          0.000\n",
      "\n",
      "    var_fac[2] = 267.273\n",
      "    x_est[2] = \n",
      "      36916.951\n",
      "   -5514040.693\n",
      "    3194170.851\n",
      "          0.000\n",
      "\n",
      "\n",
      "    x_est[2] - xyz = \n",
      "        344.625\n",
      "        194.169\n",
      "       -178.329\n",
      "\n",
      "\n",
      "The DOPs\n",
      "PDOP = 1.0\n",
      "HDOP = 0.7\n",
      "VDOP = 0.7\n",
      "TDOP = 0.0\n"
     ]
    }
   ],
   "source": [
    "## 6.7 Solve x_est iteratively\n",
    "\n",
    "## 6.0 Calculate the Covariance Matrix 𝐶𝑙 of the Observations 𝑙\n",
    "var_fac = [var_a_priori]\n",
    "Cl = []\n",
    "Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "\n",
    "## 6.1 Calculate the Weight Matrix P\n",
    "P = [inv(Cl[-1])]\n",
    "\n",
    "## 6.2 Get the design matrix and misclosure vectors using the current estimate x_est[-1] and l_obs\n",
    "w = []  # Empty list to hold the misclosure vectors\n",
    "A = []  # Empty list to hold the design matrices\n",
    "Cx = [] # Empty list to hold the covariance matrices Cx\n",
    "\n",
    "# Add the initial estimate to the list x_est\n",
    "x_est = [xyzt_0]\n",
    "\n",
    "# Get the design A and misclosures w for x_est\n",
    "result = get_design_misclosure_range( x_est[-1], eph,l_obs, c, False)\n",
    "w.append(result[0])\n",
    "A.append(result[1])\n",
    "\n",
    "## 6.3 Calculate Adjustment Variance Matrix\n",
    "Cd = [] # Empty list to hold the covariance matrices Cd\n",
    "\n",
    "# Determine the covariance Cd of the *current* estimate of x\n",
    "Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "\n",
    "## 6.4 Estimate the Adjustment Vector delta\n",
    "# Create the list of the adjustments delta\n",
    "delta = []\n",
    "\n",
    "# Add the first adjustment to the list\n",
    "delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "\n",
    "## 6.5 Estimate the Residual Vector\n",
    "r = []\n",
    "r.append(A[-1]@delta[-1]+w[-1]) # list of residual vectors\n",
    "\n",
    "## 6.5 Estimate the Residual Vector\n",
    "r = []\n",
    "r.append(A[-1]@delta[-1]+w[-1]) # list of residual vectors\n",
    "\n",
    "##6.7.0 Create the Iterative Loop in which 𝑥⃗ is estimated\n",
    "\n",
    "# Set the converge and max iteration criteria\n",
    "conv_crit = 0.0001  # Set the appropriate convergence criterion\n",
    "max_iter = 10        # The maximum number of iterations before we move on\n",
    "\n",
    "\n",
    "i = 0\n",
    "done = False\n",
    "while not done:\n",
    "    i += 1\n",
    "    # Update the estimate i.e., Create the next estimate\n",
    "    x_est.append(x_est[-1]+delta[-1])\n",
    "    \n",
    "    # There are now i+1 estimates for position, but only i for \n",
    "    # A, w, var_fac, Cx Cl, P, Cd, delta and r for the first estimate\n",
    "    # At this point update all of them \n",
    "    \n",
    "    # Update var_fac using the equations from 6.6\n",
    "    var_fac.append(  r[-1].T@P[-1]@r[-1]/(n_svs-len(x_est[-1])))\n",
    "    \n",
    "    # Update Cl and P (same way as in intial step)\n",
    "    Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "    P.append(inv(Cl[-1]))\n",
    "    \n",
    "    # Update A and w (same manner as before)\n",
    "    result = get_design_misclosure_range( x_est[-1], eph, l_obs, c, False)\n",
    "    w.append(result[0])\n",
    "    A.append(result[1])\n",
    "    \n",
    "    # Update the adjustment covariance matrix\n",
    "    Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "    \n",
    "    # Determine the new adjustment vector\n",
    "    delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "    \n",
    "    # Estimate the residuals associated to the current delta (thus the next est of x)\n",
    "    r.append(A[-1]@delta[-1]+w[-1])\n",
    "    \n",
    "    # Check whether the solution has converged by checking the residual estimates for dithering\n",
    "    if r[-1].T@r[-1] > r[-2].T@r[-2]:\n",
    "        done = True\n",
    "        if verbose:\n",
    "            print('The solution has converged, this is a rare occurence')\n",
    "    elif sqrt(delta[-1].T@delta[-1]) < conv_crit: \n",
    "        done = True\n",
    "        if verbose:\n",
    "            print('The convergence criterion of %.1fmm was reached in %d steps'%(1000*conv_crit,i))\n",
    "    elif i>=max_iter:\n",
    "        if verbose:\n",
    "            print('No convergence, max iterations reached')\n",
    "            \n",
    "# The difference between the true x and our estimate\n",
    "error_xyz = x_est[-1][0:3]-xyz\n",
    "            \n",
    "if verbose:\n",
    "    for i in np.arange(len(x_est)):\n",
    "        print('\\n    var_fac[%d] = %.3f'%(i,var_fac[i]))\n",
    "        print('    x_est[%d] = '%i)\n",
    "        for j in range(len( x_est[-1])):\n",
    "            print( '%15.3f'%(x_est[-1][j]))  \n",
    "    print('\\n\\n    x_est[%d] - xyz = '%i)\n",
    "    for i in range(len(xyz)):\n",
    "        print( '%15.3f'%(error_xyz[i]))\n",
    "    print('\\n\\nThe DOPs')\n",
    "    idx=np.ones((n_svs), dtype=bool)\n",
    "    idx[:]=True\n",
    "    A[-1]=A[-1][idx,:]\n",
    "    dop = np.diag(inv(A[-1].T@A[-1]))\n",
    "\n",
    "    print('PDOP = %.1f'%sqrt(sum(dop[0:3]**2)))\n",
    "    print('HDOP = %.1f'%sqrt(sum(dop[0:2]**2)))\n",
    "    print('VDOP = %.1f'%sqrt(dop[2]**2))\n",
    "    print('TDOP = %.1f'%sqrt(dop[3]**2))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_est[-1][0:3]-xyz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the variance factor you will see that there is some interesting behavior: namely the initial value of `var_fac`  identical to the a-priory variance factor estimate, simply because we assigned it that value. It then gets much larger as our first position estimate is not quite correct. Once the estimate of the unknown is converged (`x_est[-1]`) however the variance factor closely resembles the initial variance factor. This is to be expected as we determined the a-priori variance from what we knew about how we simulated the observations. The variance factor becomes a tool to evaluate the manufacturers claims about measurement uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_r = max(abs(r[-1]))\n",
    "idx = max_r==abs(r[-1])\n",
    "std_res = np.std(abs(r[-1][~idx]))\n",
    "if max_r < 3*std_res:\n",
    "    idx[idx]=False\n",
    "\n",
    "bv_est = [i for i,ii in enumerate(idx) if ii == True]\n",
    "bv_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution has converged\n",
      "\n",
      "    var_fac[0] = 267.274\n",
      "    x_est[0] = \n",
      "      36566.397\n",
      "   -5514262.062\n",
      "    3194354.289\n",
      "          0.000\n",
      "\n",
      "    var_fac[1] = 0.312\n",
      "    x_est[1] = \n",
      "      36566.397\n",
      "   -5514262.062\n",
      "    3194354.289\n",
      "          0.000\n",
      "\n",
      "\n",
      "    x_est[1] - xyz = \n",
      "         -5.930\n",
      "        -27.200\n",
      "          5.108\n",
      "\n",
      "\n",
      "The DOPs\n",
      "PDOP = 1.1\n",
      "HDOP = 0.8\n",
      "VDOP = 0.7\n",
      "TDOP = 0.0\n"
     ]
    }
   ],
   "source": [
    "## 6.8 Eliminate the blunder\n",
    "\n",
    "## 6.0 Calculate the Covariance Matrix 𝐶𝑙 of the Observations 𝑙\n",
    "var_fac = [var_a_priori]\n",
    "Cl = []\n",
    "Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "\n",
    "## 6.1 Calculate the Weight Matrix P\n",
    "P = [inv(Cl[-1])]\n",
    "if len(bv_est):\n",
    "    P[-1][bv_est,bv_est]=0\n",
    "\n",
    "## 6.2 Get the design matrix and misclosure vectors using the current estimate x_est[-1] and l_obs\n",
    "w = []  # Empty list to hold the misclosure vectors\n",
    "A = []  # Empty list to hold the design matrices\n",
    "Cx = [] # Empty list to hold the covariance matrices Cx\n",
    "\n",
    "# Add the initial estimate to the list x_est\n",
    "x_est = [xyzt_0]\n",
    "\n",
    "# Get the design A and misclosures w for x_est\n",
    "result = get_design_misclosure_range( x_est[-1], eph,l_obs, c, False)\n",
    "w.append(result[0])\n",
    "A.append(result[1])\n",
    "\n",
    "## 6.3 Calculate Adjustment Variance Matrix\n",
    "Cd = [] # Empty list to hold the covariance matrices Cd\n",
    "\n",
    "# Determine the covariance Cd of the *current* estimate of x\n",
    "Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "\n",
    "## 6.4 Estimate the Adjustment Vector delta\n",
    "# Create the list of the adjustments delta\n",
    "delta = []\n",
    "\n",
    "# Add the first adjustment to the list\n",
    "delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "\n",
    "## 6.5 Estimate the Residual Vector\n",
    "r = []\n",
    "r.append(A[-1]@delta[-1]+w[-1]) # list of residual vectors\n",
    "\n",
    "## 6.5 Estimate the Residual Vector\n",
    "r = []\n",
    "r.append(A[-1]@delta[-1]+w[-1]) # list of residual vectors\n",
    "\n",
    "##6.7.0 Create the Iterative Loop in which 𝑥⃗ is estimated\n",
    "\n",
    "# Set the converge and max iteration criteria\n",
    "conv_crit = 0.0001  # Set the appropriate convergence criterion\n",
    "max_iter = 10        # The maximum number of iterations before we move on\n",
    "\n",
    "\n",
    "i = 0\n",
    "done = False\n",
    "while not done:\n",
    "    i += 1\n",
    "    # Update the estimate i.e., Create the next estimate\n",
    "    x_est.append(x_est[-1]+delta[-1])\n",
    "    \n",
    "    # There are now i+1 estimates for position, but only i for \n",
    "    # A, w, var_fac, Cx Cl, P, Cd, delta and r for the first estimate\n",
    "    # At this point update all of them \n",
    "    \n",
    "    # Update var_fac using the equations from 6.6\n",
    "    var_fac.append(  r[-1].T@P[-1]@r[-1]/(n_svs-len(x_est[-1])))\n",
    "    \n",
    "    # Update Cl and P (same way as in intial step)\n",
    "    Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "    P.append(inv(Cl[-1]))\n",
    "    \n",
    "    # Eliminate the blunder vehicle\n",
    "    if len(bv_est):\n",
    "        P[-1][bv_est,bv_est]=0\n",
    "    \n",
    "    # Update A and w (same manner as before)\n",
    "    result = get_design_misclosure_range( x_est[-1], eph, l_obs, c, False)\n",
    "    w.append(result[0])\n",
    "    A.append(result[1])\n",
    "    \n",
    "    # Update the adjustment covariance matrix\n",
    "    Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "    \n",
    "    # Determine the new adjustment vector\n",
    "    delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "    \n",
    "    # Estimate the residuals associated to the current delta (thus the next est of x)\n",
    "    r.append(A[-1]@delta[-1]+w[-1])\n",
    "\n",
    "    \n",
    "    # Check whether the solution has converged by checking the residual estimates for dithering\n",
    "    if r[-1].T@r[-1] > r[-2].T@r[-2]:\n",
    "        done = True\n",
    "        if verbose:\n",
    "            print('The solution has converged')\n",
    "    elif sqrt(delta[-1].T@delta[-1]) < conv_crit: \n",
    "        done = True\n",
    "        if verbose:\n",
    "            print('The convergence criterion of %.1fmm was reached in %d steps'%(1000*conv_crit,i))\n",
    "    elif i>=max_iter:\n",
    "        done = True\n",
    "        if verbose:\n",
    "            print('No convergence, max iterations reached')\n",
    "            \n",
    "    \n",
    "            \n",
    "# The difference between the true x and our estimate\n",
    "error_xyz = x_est[-1][0:3]-xyz\n",
    "            \n",
    "\n",
    "    \n",
    "if verbose:\n",
    "    for i in np.arange(len(x_est)):\n",
    "        print('\\n    var_fac[%d] = %.3f'%(i,var_fac[i]))\n",
    "        print('    x_est[%d] = '%i)\n",
    "        for j in range(len( x_est[-1])):\n",
    "            print( '%15.3f'%(x_est[-1][j]))  \n",
    "    print('\\n\\n    x_est[%d] - xyz = '%i)\n",
    "    for i in range(len(xyz)):\n",
    "        print( '%15.3f'%(error_xyz[i]))\n",
    "    print('\\n\\nThe DOPs')\n",
    "    idx=np.ones((n_svs), dtype=bool)\n",
    "    idx[bv_est]=False\n",
    "    A[-1]=A[-1][idx,:]\n",
    "    dop = np.diag(inv(A[-1].T@A[-1]))\n",
    "\n",
    "    print('PDOP = %.1f'%sqrt(sum(dop[0:3]**2)))\n",
    "    print('HDOP = %.1f'%sqrt(sum(dop[0:2]**2)))\n",
    "    print('VDOP = %.1f'%sqrt(dop[2]**2))\n",
    "    print('TDOP = %.1f'%sqrt(dop[3]**2))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convergence criterion of 0.1mm was reached in 2 steps\n",
      "Largest residual 1524.23 for GPS satellite 1 is an outlier: \n",
      "--RAIM analysis continuing\n",
      "\n",
      "The solution has converged\n",
      "Largest residual 18.61 for GPS satellite 9 is an outlier: \n",
      "--RAIM analysis continuing\n",
      "\n",
      "The convergence criterion of 0.1mm was reached in 5 steps\n",
      "Largest residual 6.25 for GLONASS satellite 2 is an outlier: \n",
      "--RAIM analysis continuing\n",
      "\n",
      "The solution has converged\n",
      "Largest residual 2.82 for GLONASS satellite 2 is an outlier: \n",
      "--RAIM analysis continuing\n",
      "\n",
      "The solution has converged\n",
      "Largest residual 1.00 for GLONASS satellite 8 is not an outlier: \n",
      "--RAIM analysis Done\n",
      "\n",
      "    var_fac[0] = 267.274\n",
      "    x_est[0] = \n",
      "      36569.601\n",
      "   -5514244.070\n",
      "    3194350.156\n",
      "          0.000\n",
      "\n",
      "    var_fac[1] = 0.001\n",
      "    x_est[1] = \n",
      "      36569.601\n",
      "   -5514244.070\n",
      "    3194350.156\n",
      "          0.000\n",
      "\n",
      "    var_fac[2] = 263.668\n",
      "    x_est[2] = \n",
      "      36569.601\n",
      "   -5514244.070\n",
      "    3194350.156\n",
      "          0.000\n",
      "\n",
      "\n",
      "    x_est[2] - xyz = \n",
      "         -2.726\n",
      "         -9.208\n",
      "          0.975\n",
      "\n",
      "\n",
      "The DOPs\n",
      "PDOP = 2.4\n",
      "HDOP = 2.2\n",
      "VDOP = 0.8\n",
      "TDOP = 0.0\n"
     ]
    }
   ],
   "source": [
    "## 6.8 Eliminate the blunder(s) iteratively\n",
    "\n",
    "###### NOT QUITE DONE YET _ DO NOT INCLUDE IN ASSIGNMENT!!!! ####\n",
    "\n",
    "idx=np.zeros((n_svs,1), dtype=bool)\n",
    "bv_est = []\n",
    "raim = True\n",
    "i = 0\n",
    "while raim:\n",
    "\n",
    "    ## 6.0 Calculate the Covariance Matrix 𝐶𝑙 of the Observations 𝑙\n",
    "    var_fac = [var_a_priori]\n",
    "    Cl = []\n",
    "    Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "\n",
    "    ## 6.1 Calculate the Weight Matrix P\n",
    "    P = [inv(Cl[-1])]\n",
    "    P[-1][bv_est,bv_est]=0\n",
    "\n",
    "    ## 6.2 Get the design matrix and misclosure vectors using the current estimate x_est[-1] and l_obs\n",
    "    w = []  # Empty list to hold the misclosure vectors\n",
    "    A = []  # Empty list to hold the design matrices\n",
    "    Cx = [] # Empty list to hold the covariance matrices Cx\n",
    "\n",
    "    # Add the initial estimate to the list x_est\n",
    "    x_est = [xyzt_0]\n",
    "\n",
    "    # Get the design A and misclosures w for x_est\n",
    "    result = get_design_misclosure_range( x_est[-1], eph,l_obs, c, False)\n",
    "    w.append(result[0])\n",
    "    A.append(result[1])\n",
    "\n",
    "    ## 6.3 Calculate Adjustment Variance Matrix\n",
    "    Cd = [] # Empty list to hold the covariance matrices Cd\n",
    "\n",
    "    # Determine the covariance Cd of the *current* estimate of x\n",
    "    Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "\n",
    "    ## 6.4 Estimate the Adjustment Vector delta\n",
    "    # Create the list of the adjustments delta\n",
    "    delta = []\n",
    "\n",
    "    # Add the first adjustment to the list\n",
    "    delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "\n",
    "    ## 6.5 Estimate the Residual Vector\n",
    "    r = []\n",
    "    r.append(A[-1]@delta[-1]+w[-1]) # list of residual vectors\n",
    "\n",
    "    ## 6.5 Estimate the Residual Vector\n",
    "    r = []\n",
    "    r.append(A[-1]@delta[-1]+w[-1]) # list of residual vectors\n",
    "\n",
    "    ##6.7.0 Create the Iterative Loop in which 𝑥⃗ is estimated\n",
    "\n",
    "    # Set the converge and max iteration criteria\n",
    "    conv_crit = 0.0001  # Set the appropriate convergence criterion\n",
    "    max_iter = 10        # The maximum number of iterations before we move on\n",
    "\n",
    "\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        i += 1\n",
    "        # Update the estimate i.e., Create the next estimate\n",
    "        x_est.append(x_est[-1]+delta[-1])\n",
    "\n",
    "        # There are now i+1 estimates for position, but only i for \n",
    "        # A, w, var_fac, Cx Cl, P, Cd, delta and r for the first estimate\n",
    "        # At this point update all of them \n",
    "\n",
    "        # Update var_fac using the equations from 6.6\n",
    "        var_fac.append(  r[-1].T@P[-1]@r[-1]/(n_svs-len(x_est[-1])))\n",
    "\n",
    "        # Update Cl and P (same way as in intial step)\n",
    "        Cl.append(var_fac[-1] * np.eye(n_svs))\n",
    "        P.append(inv(Cl[-1]))\n",
    "\n",
    "        # Eliminate the blunder vehicle\n",
    "\n",
    "        P[-1][bv_est,bv_est]=0\n",
    "\n",
    "        # Update A and w (same manner as before)\n",
    "        result = get_design_misclosure_range( x_est[-1], eph, l_obs, c, False)\n",
    "        w.append(result[0])\n",
    "        A.append(result[1])\n",
    "\n",
    "        # Update the adjustment covariance matrix\n",
    "        Cd.append(inv(A[-1].T@P[-1]@A[-1]))\n",
    "\n",
    "        # Determine the new adjustment vector\n",
    "        delta.append(-Cd[-1]@A[-1].T@P[-1]@w[-1])\n",
    "\n",
    "        # Estimate the residuals associated to the current delta (thus the next est of x)\n",
    "        r.append(A[-1]@delta[-1]+w[-1])\n",
    "\n",
    "        # Check whether the solution has converged by checking the residual estimates for dithering\n",
    "        if r[-1].T@r[-1] > r[-2].T@r[-2]:\n",
    "            done = True\n",
    "            if verbose:\n",
    "                print('The solution has converged')\n",
    "        elif sqrt(delta[-1].T@delta[-1]) < conv_crit: \n",
    "            done = True\n",
    "            if verbose:\n",
    "                print('The convergence criterion of %.1fmm was reached in %d steps'%(1000*conv_crit,i))\n",
    "        elif i>=max_iter:\n",
    "            done = True\n",
    "            if verbose:\n",
    "                print('No convergence, max iterations reached')\n",
    "    max_r = max(abs(r[-1][~idx]))\n",
    "    mean_r = np.mean(abs(r[-1][~idx]))\n",
    "    std_r = np.std(abs(r[-1][~idx]))\n",
    "    idx = np.logical_or(idx,max_r==abs(r[-1]))\n",
    "    bv_est = [k for k,ii in enumerate(idx) if ii == True]\n",
    "    \n",
    "    if (max_r - mean_r)/std_r < 2:\n",
    "        raim = False\n",
    "        if verbose:\n",
    "            if eph[bv_est[-1],0] == 1:\n",
    "                print('Largest residual %.2f for GPS satellite %d is not an outlier: '% (max_r, eph[bv_est[-1],1]))\n",
    "            elif eph[bv_est[-1],0] == 2:\n",
    "                print('Largest residual %.2f for GLONASS satellite %d is not an outlier: '% (max_r, eph[bv_est[-1],1]))\n",
    "            print('--RAIM analysis Done')\n",
    "    elif sum(~idx) <= 5:\n",
    "        raim = False\n",
    "        print('5 satellites or less remaining for analysis: aborting solution')\n",
    "        print('--RAIM analysis Done')\n",
    "    else: #(max_r - mean_r)/std_r >= 3:\n",
    "        if verbose:\n",
    "            if eph[bv_est[-1],0] == 1:\n",
    "                print('Largest residual %.2f for GPS satellite %d is an outlier: '% (max_r, eph[bv_est[-1],1]))\n",
    "            elif eph[bv_est[-1],0] == 2:\n",
    "                print('Largest residual %.2f for GLONASS satellite %d is an outlier: '% (max_r, eph[bv_est[-1],1]))\n",
    "            print('--RAIM analysis continuing\\n')\n",
    "# The difference between the true x and our estimate\n",
    "error_xyz = x_est[-1][0:3]-xyz\n",
    "            \n",
    "if verbose:\n",
    "    for i in np.arange(len(x_est)):\n",
    "        print('\\n    var_fac[%d] = %.3f'%(i,var_fac[i]))\n",
    "        print('    x_est[%d] = '%i)\n",
    "        for j in range(len( x_est[-1])):\n",
    "            print( '%15.3f'%(x_est[-1][j]))  \n",
    "    print('\\n\\n    x_est[%d] - xyz = '%i)\n",
    "    for i in range(len(xyz)):\n",
    "        print( '%15.3f'%(error_xyz[i]))\n",
    "    print('\\n\\nThe DOPs')\n",
    "    idx=np.ones((n_svs), dtype=bool)\n",
    "    idx[bv_est[0:-1]]=False\n",
    "    A[-1]=A[-1][idx,:]\n",
    "    dop = np.diag(inv(A[-1].T@A[-1]))\n",
    "\n",
    "    print('PDOP = %.1f'%sqrt(sum(dop[0:3]**2)))\n",
    "    print('HDOP = %.1f'%sqrt(sum(dop[0:2]**2)))\n",
    "    print('VDOP = %.1f'%sqrt(dop[2]**2))\n",
    "    print('TDOP = %.1f'%sqrt(dop[3]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
